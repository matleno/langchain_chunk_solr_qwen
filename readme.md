python -m llama_cpp.server   --model /mnt/d/modelligguf/qwen2.5-7b-instruct-q3_k_m.gguf   --host 0.0.0.0   --port 8000   --n_ctx 6000   --n_threads 12   --n_batch 512  --use_mlock True